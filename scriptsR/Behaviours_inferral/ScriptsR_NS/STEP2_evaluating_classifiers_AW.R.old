rm(list=ls())

library(lme4)
library(car)
library(multcomp)

outcomes <- read.table("/home/cf19810/Documents/MachineLearning_outcomes/quality_scores.txt",header=T,stringsAsFactors = F)
beta <- 0.5
outcomes <- outcomes[which(outcomes$beta==beta),]

outcomes$classif <- outcomes$classifier #AW
outcomes_scores <- outcomes[which(names(outcomes) %in% names(outcomes)[which(grepl("test",names(outcomes))| grepl("training",names(outcomes)))])]
outcomes_param  <- outcomes[which(!names(outcomes) %in% c(names(outcomes)[which(grepl("pre_classifier",names(outcomes))| grepl("test",names(outcomes))| grepl("training",names(outcomes)))],"proportion_truegrooming_detected","Loop_ID"))]
#AW:added "classifer" to excluded outcomes_param
outcomes_to_keep <- outcomes
for (parameter in c("CAPSULE_FILE","MAX_INTERACTION_GAP",names(outcomes_param)[which(!names(outcomes_param)%in% c("CAPSULE_FILE","MAX_INTERACTION_GAP"))])){
  print(paste("Evaluating best values for parameter",parameter))
  if (length(unique(outcomes_param[,parameter]))>1){
    for (what in c("training","test")){ ####Best parameter values for training and for test, on average for all attempts
      ###Build data frame for stats
      dat <- data.frame(
                  Fbeta= outcomes_scores[,paste("Fbeta_",what,sep="")],
                   parameter = as.factor(outcomes_param[,parameter]),
                   other_params = interaction(outcomes_param[names(outcomes_param)!=parameter],sep=".")
      )
      ###Fit stats model
      model <- lmer (Fbeta~ parameter + (1|other_params),data=dat)
      
      ###List parameter values in decreasing beta_coefficient order (higher coefficient in model = better)
      coefs         <- data.frame(summary(model)$coefficients)
      best_to_worse <-   c( levels(dat$parameter)[1], gsub("parameter","",rownames(coefs)[2:nrow(coefs)]   )  )[ order(-c(0,coefs$Estimate[2:nrow(coefs)]))]
      
      ###perform Tukey post-hoc comparisons to find out which parameter values are significantly different from which, and order from best to lest good parameter value
      Tukey_cld <- cld(summary(glht(model, linfct = mcp(parameter = "Tukey"))))$mcletters$Letters[best_to_worse]
      print(Tukey_cld)
      ###keep candidates: those that share the first letter in Tukey_cld
      assign ( paste("list_",what,sep="") ,  names(Tukey_cld[which(grepl(Tukey_cld[1],Tukey_cld))]))
      rm(list=c("Tukey_cld","model","coefs","best_to_worse"))
    }
    ###then keep the values that are best for both training and test
    assign(paste(parameter,"_list",sep=""),     list_training[which(list_training %in% list_test)])
    
    ###reduce outcomes table
    if (parameter %in% c("CAPSULE_FILE","MAX_INTERACTION_GAP")){
      outcomes_scores <- outcomes_scores[which(outcomes_param[,parameter]%in%list_training[which(list_training %in% list_test)]),]
      outcomes_param  <- outcomes_param[which(outcomes_param[,parameter]%in%list_training[which(list_training %in% list_test)]),]
    }
    rm(list=c("list_test","list_training"))
  }else{
    assign(paste(parameter,"_list",sep=""),unique(outcomes_param[,parameter]))
  }
  outcomes_to_keep <- outcomes_to_keep[which(as.character(outcomes_to_keep[,parameter])  %in% as.character( get(paste(parameter,"_list",sep=""))) ), ]
}

###selected method: keep the ones with highest Fbeta_training and Fbeta_training
outcomes_to_keep <- outcomes_to_keep[which(outcomes_to_keep$CSI_training==max(outcomes_to_keep$CSI_training,na.rm=T)),]
outcomes_to_keep <- outcomes_to_keep[which(outcomes_to_keep$Fbeta_training==max(outcomes_to_keep$Fbeta_training,na.rm=T)),] #AW
###among those, keep the one that has the best generalisation value
outcomes_to_keep <- outcomes_to_keep[which(outcomes_to_keep$Fbeta_test==max(outcomes_to_keep$Fbeta_test,na.rm=T)),]
print(outcomes_to_keep)

write.table(outcomes_to_keep,file="/home/cf19810/Documents/Ants_behaviour_analysis/Data/MachineLearning_outcomes/quality_scores_CHOSEN.txt",col.names=T,row.names=F,quote=F,append=F)
